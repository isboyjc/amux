/**
 * Provider Passthrough Proxy Handler
 * 
 * Handles requests to providers configured as passthrough proxies.
 * Uses llm-bridge with Inbound = Outbound (same adapter) for minimal overhead
 * while maintaining automatic Token statistics, error handling, and streaming.
 */

import { randomUUID } from 'crypto'

import { Bridge, type Usage } from '@amux/llm-bridge'
import type { FastifyRequest, FastifyReply } from 'fastify'

import { decryptApiKey } from '../crypto'
import type { ProviderRow } from '../database/types'
import { logRequest } from '../logger'
import { recordRequest as recordMetrics } from '../metrics'

import { getAdapter } from './bridge-manager'
import { ProxyErrorCode } from './types'
import { extractApiKey, validateApiKey, createErrorResponse, getEndpointForAdapter } from './utils'

/**
 * ğŸ†• è·å– Provider çš„è®¤è¯ Tokenï¼ˆæ”¯æŒ OAuth å’Œ API Keyï¼‰
 * 
 * @param provider - Provider configuration
 * @returns Token string or { token, accountId, metadata } for OAuth providers, or null if not available
 */
async function getProviderToken(
  provider: ProviderRow
): Promise<string | { token: string; accountId: string; metadata: Record<string, unknown> } | null> {
  // æ–°æ¶æ„ï¼šæ‰€æœ‰ Providerï¼ˆåŒ…æ‹¬ OAuth Poolï¼‰éƒ½ç›´æ¥ä½¿ç”¨ api_key å­—æ®µ
  // OAuth Pool Provider çš„ api_key å­˜å‚¨çš„æ˜¯ OAuth æœåŠ¡çš„ API Key (sk-amux.oauth.codex-xxx)
  // è´¦å·é€‰æ‹©å’Œ Token ç®¡ç†ç”± OAuth è½¬æ¢æœåŠ¡å±‚å¤„ç†
  
  if (!provider.api_key) {
    return null
  }
  
  const decryptedKey = decryptApiKey(provider.api_key)
  if (!decryptedKey) {
    console.error(`[Passthrough] Failed to decrypt API key for provider: ${provider.name}`)
    return null
  }
  
  return decryptedKey
}

/**
 * Handle Provider Passthrough Proxy request
 * 
 * This function:
 * 1. Validates authentication
 * 2. Creates a Bridge with Inbound = Outbound (same adapter)
 * 3. Handles both streaming and non-streaming requests
 * 4. Automatically collects Token statistics from Bridge
 * 5. Records metrics and logs
 */
export async function handleProviderPassthrough(
  request: FastifyRequest,
  reply: FastifyReply,
  provider: ProviderRow
): Promise<void> {
  // Detect request source (local vs tunnel)
  const isTunnelRequest = !!(
    request.headers['cf-ray'] || 
    request.headers['cf-connecting-ip'] ||
    request.headers['cf-visitor']
  )
  const requestSource: 'local' | 'tunnel' = isTunnelRequest ? 'tunnel' : 'local'
  const requestId = randomUUID()
  const startTime = Date.now()
  const errorFormat = provider.adapter_type === 'anthropic' ? 'anthropic' : 'openai'
  
  console.log(`\n[Passthrough] ğŸš€ Handling request for provider: ${provider.name}`)
  console.log(`[Passthrough]   - Request URL: ${request.url}`)
  console.log(`[Passthrough]   - Request method: ${request.method}`)
  console.log(`[Passthrough]   - Provider adapter: ${provider.adapter_type}`)
  console.log(`[Passthrough]   - Provider base_url: ${provider.base_url}`)
  console.log(`[Passthrough]   - Provider chat_path: ${provider.chat_path}`)
  
  try {
    const body = request.body as any
    
    console.log(`[Passthrough]   - Request params:`, request.params)
    console.log(`[Passthrough]   - Request body model (before): ${body.model}`)
    
    // ğŸ†• å¯¹äº Google adapterï¼Œä» URL å‚æ•°ä¸­æå–æ¨¡å‹åå¹¶æ³¨å…¥åˆ°è¯·æ±‚ä½“
    // URL æ ¼å¼ï¼š/providers/{path}/v1beta/models/{model}:streamGenerateContent
    if (provider.adapter_type === 'google' && !body.model) {
      const params = request.params as any
      
      console.log(`[Passthrough]   - Google adapter: extracting model from params`)
      
      // æ”¯æŒä¸¤ç§è·¯ç”±æ ¼å¼ï¼š
      // 1. é€šé…ç¬¦ï¼šparams['*'] = 'gemini-2.5-flash-lite:streamGenerateContent'
      // 2. è·¯ç”±å‚æ•°ï¼šparams.model = 'gemini-2.5-flash-lite'
      let modelName: string | undefined
      
      if (params['*']) {
        // ä»é€šé…ç¬¦ä¸­æå–æ¨¡å‹åï¼ˆå†’å·ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
        const wildcardParam = params['*'] as string
        console.log(`[Passthrough]   - Wildcard param: ${wildcardParam}`)
        const colonIndex = wildcardParam.indexOf(':')
        modelName = colonIndex > 0 ? wildcardParam.substring(0, colonIndex) : wildcardParam
        console.log(`[Passthrough]   - Extracted model from wildcard: ${modelName}`)
      } else if (params.model) {
        modelName = params.model
        console.log(`[Passthrough]   - Model from route param: ${modelName}`)
      }
      
      if (modelName) {
        body.model = modelName
        console.log(`[Passthrough]   - Injected model into body: ${modelName}`)
      }
    }
    
    console.log(`[Passthrough]   - Request body model (after): ${body.model}`)

    // 1. Detect internal requests (from Chat IPC - localhost + no auth header)
    const apiKey = extractApiKey(request)
    const isInternalRequest = requestSource === 'local' && !apiKey

    // 2. Determine which API key to use
    let targetApiKey: string

    if (isInternalRequest) {
      // Internal request - use provider's token (OAuth or API Key)

      const result = await getProviderToken(provider)  // âœ… ä½¿ç”¨æ–°å‡½æ•°
      if (!result) {
        const error = createErrorResponse(
          ProxyErrorCode.MISSING_API_KEY,
          `Provider "${provider.name}" has no API key or OAuth account configured.`,
          500,
          errorFormat
        )
        return reply.status(error.statusCode).send(error.body)
      }

      targetApiKey = result
    } else {
      // External request - validate and use appropriate key
      const keyValidation = validateApiKey(apiKey)

      if (!keyValidation.valid) {
        const error = createErrorResponse(
          keyValidation.error?.includes('required') ? ProxyErrorCode.MISSING_API_KEY : ProxyErrorCode.INVALID_API_KEY,
          keyValidation.error || 'Invalid API key',
          401,
          errorFormat
        )
        return reply.status(error.statusCode).send(error.body)
      }

      if (keyValidation.usePassThrough) {
        // User provided their own key (pass-through mode)
        targetApiKey = apiKey!
      } else {
        // Use provider's token (OAuth or API Key)
        const result = await getProviderToken(provider)  // âœ… ä½¿ç”¨æ–°å‡½æ•°
        if (!result) {
          const error = createErrorResponse(
            ProxyErrorCode.MISSING_API_KEY,
            `Provider "${provider.name}" has no API key configured.`,
            500,
            errorFormat
          )
          return reply.status(error.statusCode).send(error.body)
        }

        targetApiKey = result
      }
    }
    
    // 3. Get Adapter (Inbound = Outbound)
    const adapter = getAdapter(provider.adapter_type)
    if (!adapter) {
      throw new Error(`Adapter not found: ${provider.adapter_type}`)
    }
    
    // 4. å¤„ç† chatPath ä¸­çš„ {model} å ä½ç¬¦
    // å¯¹äº OAuth Providerï¼Œprovider.chat_path å¯èƒ½ä¸º nullï¼Œæ­¤æ—¶ llm-bridge ä¼šä½¿ç”¨ adapter çš„é»˜è®¤ chatPath
    // å¦‚æœé»˜è®¤ chatPath åŒ…å« {model} å ä½ç¬¦ï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢ï¼Œå› ä¸º llm-bridge å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†
    let chatPath = provider.chat_path
    
    if (provider.adapter_type === 'google' && !chatPath && body.model) {
      // è·å– Google adapter çš„é»˜è®¤ chatPath
      const defaultChatPath = getEndpointForAdapter('google')
      console.log(`[Passthrough]   - Google adapter default chatPath: ${defaultChatPath}`)
      
      // æ‰‹åŠ¨æ›¿æ¢ {model} å ä½ç¬¦
      if (defaultChatPath.includes('{model}')) {
        chatPath = defaultChatPath.replace('{model}', body.model)
        console.log(`[Passthrough]   - Replaced {model} with ${body.model}: ${chatPath}`)
      }
    }
    
    // 5. Create Bridge with standard adapter (OAuth translation handled by dedicated service)
    const bridgeConfig = {
      apiKey: targetApiKey,
      baseURL: provider.base_url || undefined,
      chatPath: chatPath || undefined,
      timeout: 60000,
    }
    
    console.log(`[Passthrough]   - Bridge config:`, {
      baseURL: bridgeConfig.baseURL,
      chatPath: bridgeConfig.chatPath,
      model: body.model
    })
    
    const bridge = new Bridge({
      inbound: adapter,
      outbound: adapter,  // Passthrough uses same adapter for inbound/outbound
      config: bridgeConfig,
      // â­ Add hooks to capture token usage (unified IR format!)
      hooks: {
        onResponse: async (ir) => {
          // Tokenç»Ÿè®¡å·²ç»æ˜¯ç»Ÿä¸€æ ¼å¼ï¼Œæ— éœ€åŒºåˆ† Providerï¼
          if (ir.usage) {
            ;(bridge as any)._lastUsage = ir.usage
          }
        },
        onStreamEvent: async (event) => {
          // æµå¼å“åº”ä¸­çš„ Token ä¹Ÿæ˜¯ç»Ÿä¸€æ ¼å¼
          if (event.type === 'end' && event.usage) {
            ;(bridge as any)._lastUsage = event.usage
          }
        }
      }
    })
    
    // 4. Handle request (streaming vs non-streaming)
    console.log(`[Passthrough] ğŸ“‹ Request body.stream: ${body.stream}`)
    console.log(`[Passthrough] ğŸ“‹ Request body keys: ${Object.keys(body).join(', ')}`)
    console.log(`[Passthrough] ğŸ“‹ Request URL: ${request.url}`)
    
    // åˆ¤æ–­æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
    // - OpenAI/Anthropic: ä½¿ç”¨ body.stream å­—æ®µ
    // - Google: æ£€æŸ¥ URL ä¸­çš„ alt=sse å‚æ•° æˆ– è¯·æ±‚æ–¹æ³•ååŒ…å« "stream"
    const isStreamRequest = body.stream || 
      (provider.adapter_type === 'google' && (
        request.url.includes('alt=sse') || 
        request.url.includes('stream') ||
        (chatPath && chatPath.includes('stream'))
      ))
    
    console.log(`[Passthrough] ğŸ“‹ Is stream request: ${isStreamRequest}`)
    
    if (isStreamRequest) {
      // Streaming response
      console.log(`[Passthrough] ğŸŒŠ Using STREAMING mode`)
      
      // å¯¹äº Google adapterï¼Œç¡®ä¿ body.stream è®¾ç½®ä¸º true
      // è¿™æ · llm-bridge æ‰èƒ½æ­£ç¡®å¤„ç†æµå¼å“åº”
      if (provider.adapter_type === 'google' && !body.stream) {
        body.stream = true
        console.log(`[Passthrough] âœ… Set body.stream = true for Google adapter`)
      }
      
      reply.raw.setHeader('Content-Type', 'text/event-stream')
      reply.raw.setHeader('Cache-Control', 'no-cache')
      reply.raw.setHeader('Connection', 'keep-alive')
      reply.raw.setHeader('X-Request-ID', requestId)
      
      let streamSuccess = true
      let streamError: string | undefined
      const streamChunks: unknown[] = []
      
      try {
        console.log(`[Passthrough] ğŸŒŠ Starting stream for ${provider.adapter_type}`)
        const stream = await bridge.chatStream(body)
        console.log(`[Passthrough] âœ… Stream created successfully`)
        
        let chunkCount = 0
        for await (const event of stream) {
          chunkCount++
          // Bridge returns SSE events in format: { event: "...", data: {...} }
          // Extract the actual data for passthrough
          const sseEvent = event as { event?: string; data?: unknown; type?: string }
          
          if (chunkCount === 1) {
            console.log(`[Passthrough] ğŸ“¦ First chunk type: ${sseEvent.type || sseEvent.event}`)
            console.log(`[Passthrough] ğŸ“¦ First chunk data keys: ${Object.keys(sseEvent.data || sseEvent).join(', ')}`)
          }

          // Collect chunks for logging
          streamChunks.push(sseEvent.data || sseEvent)
          
          // Format SSE based on adapter type
          if (provider.adapter_type === 'anthropic' || provider.adapter_type === 'openai-responses') {
            // Anthropic/OpenAI Responses format: event: xxx\ndata: {...}\n\n
            const eventType = sseEvent.event || sseEvent.type || 'message'
            const eventData = sseEvent.data || sseEvent
            reply.raw.write(`event: ${eventType}\ndata: ${JSON.stringify(eventData)}\n\n`)
          } else if (provider.adapter_type === 'google') {
            // Google æ ¼å¼ï¼šç›´æ¥è½¬å‘åŸå§‹äº‹ä»¶
            const chunkData = sseEvent.data || sseEvent
            reply.raw.write(`data: ${JSON.stringify(chunkData)}\n\n`)
          } else {
            // OpenAI Chat Completions format: data: {...}\n\n
            // For OpenAI format, we need to send the actual chunk data, not the wrapper
            const chunkData = sseEvent.data || sseEvent
            reply.raw.write(`data: ${JSON.stringify(chunkData)}\n\n`)
          }
        }
        
        console.log(`[Passthrough] âœ… Stream completed, total chunks: ${chunkCount}`)
        
        // Add protocol-level end marker for OpenAI Chat Completions format only
        if (provider.adapter_type !== 'anthropic' && provider.adapter_type !== 'openai-responses') {
          reply.raw.write('data: [DONE]\n\n')
          console.log(`[Passthrough] ğŸ“¤ Sent [DONE] marker`)
        }
        
        reply.raw.end()
        console.log(`[Passthrough] âœ… Stream ended successfully`)
      } catch (error) {
        streamSuccess = false
        streamError = error instanceof Error ? error.message : 'Stream error'
        console.error(`[Passthrough] âŒ Stream error:`, error)
        console.error(`[Passthrough] Error type: ${error.constructor.name}`)
        console.error(`[Passthrough] Error message: ${streamError}`)
        
        // ğŸ”„ æå–åŸå§‹é”™è¯¯è¯¦æƒ…
        let errorDetails: any = {
          type: 'api_error',
          message: streamError,
          code: ProxyErrorCode.INTERNAL_ERROR
        }
        
        // å¦‚æœæ˜¯ Bridge çš„ APIErrorï¼Œæå–åŸå§‹é”™è¯¯ä¿¡æ¯
        if (error && typeof error === 'object') {
          const err = error as any
          // âœ… ç›´æ¥ä½¿ç”¨ err.dataï¼ˆå®Œæ•´çš„é”™è¯¯å“åº”ï¼‰
          if (err.data) {
            errorDetails = err.data
          } else if (err.details) {
            errorDetails = err.details
          }
        }
        
        if (provider.adapter_type === 'anthropic') {
          reply.raw.write(`event: error\ndata: ${JSON.stringify({
            type: 'error',
            error: errorDetails
          })}\n\n`)
        } else {
          reply.raw.write(`data: ${JSON.stringify({
            error: errorDetails
          })}\n\n`)
        }
        reply.raw.end()
      }
      
      const latencyMs = Date.now() - startTime
      
      // Get Token statistics from Bridge automatically
      const usage = (bridge as any)._lastUsage as Usage | undefined
      const inputTokens = usage?.promptTokens
      const outputTokens = usage?.completionTokens
      
      // Log request (including OAuth account info)
      const finalStatusCode = streamSuccess ? 200 : 500
      console.log(`[Passthrough] ğŸ“Š Logging request: success=${streamSuccess}, statusCode=${finalStatusCode}`)
      
      logRequest({
        proxyPath: provider.proxy_path || `provider-${provider.id}`,
        sourceModel: body.model,
        targetModel: body.model,
        statusCode: finalStatusCode,
        inputTokens,
        outputTokens,
        latencyMs,
        requestBody: JSON.stringify(body),
        responseBody: streamSuccess && streamChunks.length > 0 
          ? JSON.stringify({ chunks: streamChunks, totalChunks: streamChunks.length })
          : undefined,
        error: streamError,
        source: requestSource
      })
      recordMetrics(
        `provider-${provider.id}`,
        provider.id,
        streamSuccess,
        latencyMs,
        inputTokens,
        outputTokens
      )
      
      return
    }
    
    // Non-streaming response
    console.log(`[Passthrough] ğŸ“ Using NON-STREAMING mode`)
    try {
      console.log(`[Passthrough] ğŸ”„ Calling bridge.chat()...`)
      const response = await bridge.chat(body)
      console.log(`[Passthrough] âœ… bridge.chat() completed`)
      const latencyMs = Date.now() - startTime
      
      // Get Token statistics from Bridge automatically
      const usage = (bridge as any)._lastUsage as Usage | undefined
      const inputTokens = usage?.promptTokens
      const outputTokens = usage?.completionTokens
      
      // Log successful request (including OAuth account info)
      logRequest({
        proxyPath: provider.proxy_path || `provider-${provider.id}`,
        sourceModel: body.model,
        targetModel: body.model,
        statusCode: 200,
        inputTokens,
        outputTokens,
        latencyMs,
        requestBody: JSON.stringify(body),
        responseBody: JSON.stringify(response),
        source: requestSource
      })
      recordMetrics(
        `provider-${provider.id}`,
        provider.id,
        true,
        latencyMs,
        inputTokens,
        outputTokens
      )
      
      reply.header('X-Request-ID', requestId)
      return reply.send(response)
    } catch (error) {
      console.error(`[Passthrough] âŒ bridge.chat() failed:`, error)
      console.error(`[Passthrough] Error type: ${error.constructor.name}`)
      console.error(`[Passthrough] Error message: ${error instanceof Error ? error.message : 'Unknown'}`)
      
      const latencyMs = Date.now() - startTime
      const errorMessage = error instanceof Error ? error.message : 'Chat request failed'
      
      // ğŸ”„ æå–åŸå§‹çŠ¶æ€ç å’Œé”™è¯¯è¯¦æƒ…
      let statusCode = 502
      let errorBody: any = {
        message: errorMessage,
        type: 'api_error',
        code: ProxyErrorCode.ADAPTER_ERROR
      }
      
      // å¦‚æœæ˜¯ Bridge çš„ APIErrorï¼Œæå–åŸå§‹é”™è¯¯ä¿¡æ¯
      if (error && typeof error === 'object') {
        const err = error as any
        
        // Bridge çš„ APIError ç»“æ„ï¼š{ status, data, provider, details }
        if (err.status) {
          statusCode = err.status
        }
        
        // æå–é”™è¯¯è¯¦æƒ…ï¼ˆä¼˜å…ˆä½¿ç”¨å®Œæ•´çš„ dataï¼‰
        if (err.data) {
          // âœ… ç›´æ¥ä½¿ç”¨ err.dataï¼Œå®ƒå¯èƒ½æ˜¯å®Œæ•´çš„é”™è¯¯å“åº”
          errorBody = err.data
        } else if (err.details) {
          errorBody = err.details
        }
      }
      
      // Log failed request
      logRequest({
        proxyPath: provider.proxy_path || `provider-${provider.id}`,
        sourceModel: body.model,
        targetModel: body.model,
        statusCode,
        inputTokens: undefined,
        outputTokens: undefined,
        latencyMs,
        requestBody: JSON.stringify(body),
        error: errorMessage,
        source: requestSource
      })
      recordMetrics(`provider-${provider.id}`, provider.id, false, latencyMs)
      
      return reply.status(statusCode).send({ error: errorBody })
    }
  } catch (error) {
    console.error(`[Passthrough] Error:`, error)
    const err = createErrorResponse(
      ProxyErrorCode.INTERNAL_ERROR,
      error instanceof Error ? error.message : 'Internal server error',
      500,
      errorFormat
    )
    return reply.status(err.statusCode).send(err.body)
  }
}
