---
title: Anthropic Adapter
description: Use the Anthropic adapter to connect to Claude 3.5 Sonnet, Claude 3 Opus and more
---

The Anthropic adapter provides complete integration with the Anthropic Claude API, supporting the Messages API, tool use, vision capabilities, and streaming.

## Installation

```bash tab="pnpm"
pnpm add @amux.ai/llm-bridge @amux.ai/adapter-anthropic
```

```bash tab="npm"
npm install @amux.ai/llm-bridge @amux.ai/adapter-anthropic
```

## Basic Usage

```typescript
import { createBridge } from '@amux.ai/llm-bridge'
import { anthropicAdapter } from '@amux.ai/adapter-anthropic'

const bridge = createBridge({
  inbound: anthropicAdapter,
  outbound: anthropicAdapter,
  config: {
    apiKey: process.env.ANTHROPIC_API_KEY,
  },
})

const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'What is Amux?' }],
})

console.log(response.content[0].text)
```

<Callout type="info">
  If `max_tokens` is not provided, the adapter defaults it to `4096` when
  building outbound Anthropic requests.
</Callout>

## Supported Features

<Tabs items={['Tool Use', 'Vision', 'System Prompt', 'Streaming']}>
  <Tab value="Tool Use">
    ### Tool Use

    ```typescript
    const response = await bridge.chat({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages: [
        { role: 'user', content: 'What time is it in Beijing?' }
      ],
      tools: [{
        name: 'get_current_time',
        description: 'Get the current time for a specified city',
        input_schema: {
          type: 'object',
          properties: {
            city: {
              type: 'string',
              description: 'City name'
            }
          },
          required: ['city']
        }
      }]
    })

    // Check for tool calls
    if (response.stop_reason === 'tool_use') {
      const toolUse = response.content.find(c => c.type === 'tool_use')
      console.log('Tool:', toolUse.name)
      console.log('Input:', toolUse.input)
    }
    ```

  </Tab>

  <Tab value="Vision">
    ### Vision Capabilities

    ```typescript
    const response = await bridge.chat({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages: [{
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'What is in this image?'
          },
          {
            type: 'image',
            source: {
              type: 'url',
              url: 'https://example.com/image.jpg'
            }
          }
        ]
      }]
    })
    ```

    **Supported Image Formats:**
    - URL (https://)
    - Base64 encoded (requires media_type)

  </Tab>

  <Tab value="System Prompt">
    ### System Prompt

    Claude uses a separate `system` parameter:

    ```typescript
    const response = await bridge.chat({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      system: 'You are a helpful AI assistant specializing in technical questions.',
      messages: [
        { role: 'user', content: 'What is TypeScript?' }
      ]
    })
    ```

  </Tab>

  <Tab value="Streaming">
    ### Streaming

    ```typescript
    const stream = bridge.chatStreamRaw({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages: [
        { role: 'user', content: 'Tell me a story' }
      ],
      stream: true
    })

    for await (const event of stream) {
      if (event.type === 'content') {
        process.stdout.write(event.content.delta)
      }
    }
    ```

  </Tab>

  <Tab value="Extended Thinking">
    ### Extended Thinking (Reasoning)

    Claude adapter exposes reasoning capability at the Bridge capability level, but the current outbound request builder does not pass a `thinking` request field directly.

    ```typescript
    const response = await bridge.chat({
      model: 'claude-3-7-sonnet-20250219',
      max_tokens: 4096,
      messages: [
        { role: 'user', content: 'Solve this complex problem step by step...' }
      ],
      // thinking request fields are not directly mapped in current adapter request builder
    })

    // Use Bridge hooks or IR stream/response fields to observe normalized reasoning events where available
    ```

  </Tab>
</Tabs>

## Supported Models

| Model                        | Context Length | Description                    |
| ---------------------------- | -------------- | ------------------------------ |
| `claude-3-5-sonnet-20241022` | 200K           | Latest and most capable        |
| `claude-3-opus-20240229`     | 200K           | Most intelligent model         |
| `claude-3-sonnet-20240229`   | 200K           | Balanced performance and speed |
| `claude-3-haiku-20240307`    | 200K           | Fastest and most economical    |

## Configuration Options

```typescript
const bridge = createBridge({
  inbound: anthropicAdapter,
  outbound: anthropicAdapter,
  config: {
    apiKey: process.env.ANTHROPIC_API_KEY,
    baseURL: 'https://api.anthropic.com', // Optional
    timeout: 60000, // Optional
    headers: {
      'anthropic-version': '2023-06-01', // API version
    },
  },
})
```

## Converting with OpenAI Format

### OpenAI → Anthropic

```typescript
import { openaiAdapter } from '@amux.ai/adapter-openai'
import { anthropicAdapter } from '@amux.ai/adapter-anthropic'

const bridge = createBridge({
  inbound: openaiAdapter,
  outbound: anthropicAdapter,
  config: {
    apiKey: process.env.ANTHROPIC_API_KEY,
  },
})

// Send request in OpenAI format
const response = await bridge.chat({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello' }],
})
// Returns OpenAI format response
```

### Anthropic → OpenAI

```typescript
const bridge = createBridge({
  inbound: anthropicAdapter,
  outbound: openaiAdapter,
  config: {
    apiKey: process.env.OPENAI_API_KEY,
  },
})

// Send request in Anthropic format
const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'Hello' }],
})
// Returns Anthropic format response
```

## Feature Support

| Feature           | Supported | Notes                                                                           |
| ----------------- | --------- | ------------------------------------------------------------------------------- |
| Messages API      | ✅        | Fully supported                                                                 |
| Streaming         | ✅        | Fully supported                                                                 |
| Tool Use          | ✅        | Fully supported                                                                 |
| Vision            | ✅        | Claude 3 series                                                                 |
| System Prompt     | ✅        | Separate system parameter                                                       |
| Long Context      | ✅        | 200K tokens                                                                     |
| Extended Thinking | ✅        | Capability declared; direct `thinking` request passthrough is currently limited |

## Best Practices

### 1. Prefer Setting max_tokens Explicitly

```typescript
// ✅ Correct
const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 1024, // Recommended
  messages: [...]
})

// ✅ Also valid - adapter defaults to 4096
const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  messages: [...] // max_tokens omitted
})
```

### 2. Use System Prompts to Optimize Responses

```typescript
const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 1024,
  system:
    'You are a professional technical documentation writer. Use clear, concise language.',
  messages: [{ role: 'user', content: 'Explain what a REST API is' }],
})
```

### 3. Handle Long Conversations

```typescript
// Claude supports 200K tokens context
const response = await bridge.chat({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 2048,
  messages: [
    { role: 'user', content: 'First question' },
    { role: 'assistant', content: 'First answer' },
    { role: 'user', content: 'Second question' },
    { role: 'assistant', content: 'Second answer' },
    // ... can have many turns
    { role: 'user', content: 'Latest question' },
  ],
})
```

## Related Resources

- [Anthropic API Documentation](https://docs.anthropic.com/claude/reference)
- [Claude Models](https://docs.anthropic.com/claude/docs/models-overview)
- [Anthropic Pricing](https://www.anthropic.com/pricing)

## Next Steps

<Cards>
  <Card title="OpenAI Adapter" href="/en/docs/adapters/openai">
    Learn how to use GPT models
  </Card>
  <Card title="DeepSeek Adapter" href="/en/docs/adapters/deepseek">
    Learn how to use DeepSeek models
  </Card>
  <Card title="Adapter API" href="/en/docs/api/adapters">
    View the complete Adapter API reference
  </Card>
</Cards>
