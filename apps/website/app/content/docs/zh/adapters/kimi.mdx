---
title: Kimi 适配器
description: 使用 Kimi 适配器连接月之暗面的 Moonshot 系列模型
---

Kimi 适配器提供了与月之暗面（Moonshot AI）Kimi API 的集成。Kimi 以超长上下文（200K tokens）著称，API 完全兼容 OpenAI 格式。

## 安装

```bash tab="pnpm"
pnpm add @llm-bridge/core @llm-bridge/adapter-kimi
```

```bash tab="npm"
npm install @llm-bridge/core @llm-bridge/adapter-kimi
```

## 基本使用

```typescript
import { createBridge } from '@llm-bridge/core'
import { kimiAdapter } from '@llm-bridge/adapter-kimi'

const bridge = createBridge({
  inbound: kimiAdapter,
  outbound: kimiAdapter,
  config: {
    apiKey: process.env.KIMI_API_KEY
  }
})

const response = await bridge.chat({
  model: 'moonshot-v1-8k',
  messages: [
    { role: 'system', content: '你是 Kimi，由月之暗面科技提供的人工智能助手。' },
    { role: 'user', content: '什么是 LLM Bridge？' }
  ]
})

console.log(response.choices[0].message.content)
```

## 支持的模型

| 模型 | 上下文长度 | 描述 |
|------|-----------|------|
| `moonshot-v1-8k` | 8K | 标准上下文模型 |
| `moonshot-v1-32k` | 32K | 中等上下文模型 |
| `moonshot-v1-128k` | 128K | 超长上下文模型 |

<Callout type="info">
Kimi 的最大特点是支持超长上下文，适合处理长文档、长对话等场景。
</Callout>

## 主要功能

<Tabs items={['长上下文', '函数调用', '流式传输']}>
  <Tab value="长上下文">
    ### 超长上下文

    Kimi 支持最长 128K tokens 的上下文：

    ```typescript
    const response = await bridge.chat({
      model: 'moonshot-v1-128k',
      messages: [
        {
          role: 'user',
          content: `请总结以下长文档：\n\n${longDocument}`
        }
      ]
    })
    ```

    **使用场景：**
    - 长文档分析和总结
    - 多轮长对话
    - 代码库分析
    - 学术论文阅读
  </Tab>

  <Tab value="函数调用">
    ### 函数调用

    ```typescript
    const response = await bridge.chat({
      model: 'moonshot-v1-8k',
      messages: [
        { role: 'user', content: '北京现在几点？' }
      ],
      tools: [{
        type: 'function',
        function: {
          name: 'get_current_time',
          description: '获取指定城市的当前时间',
          parameters: {
            type: 'object',
            properties: {
              city: { type: 'string', description: '城市名称' }
            },
            required: ['city']
          }
        }
      }]
    })
    ```
  </Tab>

  <Tab value="流式传输">
    ### 流式传输

    ```typescript
    const stream = bridge.chatStream({
      model: 'moonshot-v1-8k',
      messages: [
        { role: 'user', content: '讲一个故事' }
      ],
      stream: true
    })

    for await (const chunk of stream) {
      if (chunk.choices[0]?.delta?.content) {
        process.stdout.write(chunk.choices[0].delta.content)
      }
    }
    ```
  </Tab>
</Tabs>

## 配置选项

```typescript
const bridge = createBridge({
  inbound: kimiAdapter,
  outbound: kimiAdapter,
  config: {
    apiKey: process.env.KIMI_API_KEY,
    baseURL: 'https://api.moonshot.cn', // 默认值
    timeout: 60000
  }
})
```

## 功能支持

| 功能 | 支持 | 说明 |
|------|------|------|
| 聊天补全 | ✅ | 完全支持 |
| 流式传输 | ✅ | 完全支持 |
| 函数调用 | ✅ | 完全支持 |
| 长上下文 | ✅ | 最长 128K tokens |
| 视觉 | ❌ | 不支持 |
| 系统提示 | ✅ | 完全支持 |

## 最佳实践

### 1. 根据需求选择模型

```typescript
// 短对话使用 8K 模型（更快更便宜）
const shortChat = await bridge.chat({
  model: 'moonshot-v1-8k',
  messages: [{ role: 'user', content: '你好' }]
})

// 长文档分析使用 128K 模型
const longAnalysis = await bridge.chat({
  model: 'moonshot-v1-128k',
  messages: [
    { role: 'user', content: `分析这份长文档：\n\n${longDoc}` }
  ]
})
```

### 2. 优化长文档处理

```typescript
const response = await bridge.chat({
  model: 'moonshot-v1-128k',
  messages: [
    {
      role: 'system',
      content: '你是一个专业的文档分析助手。提供简洁、结构化的总结。'
    },
    {
      role: 'user',
      content: `请总结以下文档的要点：\n\n${document}`
    }
  ],
  temperature: 0.3 // 降低温度以获得更准确的总结
})
```

### 3. 处理多轮对话

```typescript
// Kimi 支持长对话历史
const messages = [
  { role: 'user', content: '第一个问题' },
  { role: 'assistant', content: '第一个回答' },
  // ... 可以有很多轮
  { role: 'user', content: '最新的问题' }
]

const response = await bridge.chat({
  model: 'moonshot-v1-32k',
  messages
})
```

## 与 OpenAI 互转

Kimi 完全兼容 OpenAI 格式：

```typescript
import { openaiAdapter } from '@llm-bridge/adapter-openai'
import { kimiAdapter } from '@llm-bridge/adapter-kimi'

const bridge = createBridge({
  inbound: openaiAdapter,
  outbound: kimiAdapter,
  config: {
    apiKey: process.env.KIMI_API_KEY
  }
})

// 使用 OpenAI 格式发送请求
const response = await bridge.chat({
  model: 'gpt-4',
  messages: [{ role: 'user', content: '你好' }]
})
```

## 相关资源

- [Kimi API 文档](https://platform.moonshot.cn/docs)
- [Kimi 定价](https://platform.moonshot.cn/pricing)

## 下一步

<Cards>
  <Card title="Qwen 适配器" href="/docs/zh/adapters/qwen">
    了解如何使用通义千问模型
  </Card>
  <Card title="DeepSeek 适配器" href="/docs/zh/adapters/deepseek">
    了解如何使用 DeepSeek 模型
  </Card>
</Cards>
